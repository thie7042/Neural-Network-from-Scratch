This repository will track the development of an ANN from scratch in Python. 


A quick breakdown of terminology: 

- Deep Neural Network = A Neural Network with 2 or more hidden layers 
- Supervised Machine Learning - Used when operating with pre-established data (which has been labeled) for training. 
- Features = the measurements which collectively form our samples 
- Samples = our training data that is fed into the neural network 
- Regression -> Numerical prediction 
- Unsupervised learning = ML tool finds patterns in data without knowing the labels or classes
- Input layer = our input data in numeric form [ Example: RGB values of pixels from an image / sensor data etc.] We typically need to preprocess our data before passing it to the neural network [ Normalize, scale etc. ]
- Note: Common practise to scale input to values between 0 and 1 (or -1 and 1)
- Output layer = The neural network prediction / output 
- Classification -> the neural network is attempting to predict the class of the input data. For this application, the output layer will have a neruon for each potential class (unless we are conducting a binary classification, in whcih case a single output neuron may suffice)
- Clustering -> Assigning unstructured data into groups (clusters)
- Tensors -> an object that holds data (closely related to arrays).
- "A tensor is an object that can be represented as an array"
- Homologous -> Each list along a dimension is equally long 
- Dot produce produces scalar elements (sum of products), cross product produces vectors 
- Batches -> The segmentation of data for training.
- Batching data allows for parallel processing and assists generalization during training (instead of fittign to a single sample, we can make general tweaks to fit to the broader database)
- Hidden layers -> Layers positioned between the first (input) and last (output) layers 
- Loss and Accuracy are our useful tools (accuracy describes how often the largest confidence interval is correct for categorization)

The "learning" process: 

- Dense layers (a.k.a fully connected "fc") (The most common layer type) -> each neuron is connected to each neuron of the next layer. Each neuron output value becomes an input for the next layer
- Connections between neurons contain a weight which controls how much of the neurons input is used. Connection weights are trainable through the learning process 
- Weights are mulitplied by the input value to create the input. Each of these inputs which flow into a neuron is summed together and a bias is added
- Bias = a threshold which controls the amount of 'activation' input which is required before a neuron is activated 
- A Bias can offset the input weights either positively or negatively
- Weights and Bias's are the tuned parameters which allow the nerual network to learn
- Weights (w) will control the magnitude of the input (or even flip the sign of the input)
- The bias (b) can be thought of as an offset parameter. It will shift the value of the input
- Example: x1 = w0x0 + b1   [ of the form y=mx + b . This would be a summed equation in practise ]
- Overfitting = When the neural netowrk only learns the best way to fit the data, without learning anything about the underlying input-output relationship. The ANN has just memorized the trainign data.
- Overcomming overfiting -> use "in sample" data to train and "out of sample" data to validate 
- Example: 100,000 data set. Commit 90,000 to in-sample training, 10,000 to validation 
- The goal: Our model is capable of predicting the training and validation data 
- Note: Typically weights and biases are initialized randomly. However, if we want to load a pretrained model, we will initialize using the tuned parameters 
- Why activation fucntions; To solve non-linear problems 
- The loss function / cost function -> a tool to calculate how wrong our neural network is for its current predictions 
- Ideally, we want our loss (model error) to be 0 

Activation Functions:

- Why non-linear activation functions:Linear activation functions only calculate linear relationships between the prediction y^ and the input features
- In general -> we want non-linear activation functions to induce nonlinear behaviour within our network
- Step function: Think of a binary switch. e.g if x>0, y=1 (activation), if <=0, y=0 (no activation). 
- For a step function: If a neurons output value ( the sum of weight x inuts + the bias ) is greater than 0, the neuron fires. We also want to apply an activation function to the output. 
- Step functions are a binary tool. In practise, empirical evidence has shown that other (more sophisticated) functions provide better results. An extremely common example is ReLu
- ReLU = Rectified Linear. y=x clipped at 0 from the negative side. if x <= 0, y = 0. Otherwise, y=x. Most widely used activation function (This is actually technically non-linear!)
- Sigmoid is also a very popular choice of activation function (more informative) y = 1/(1+e^-x) . Range 0->1 (-infinity -> + infinity) . Equal to 0.5 for an input of 0 
- The Linear activation  function y=x - simply a line equation (usually applied to the last layers output in regresion models).  
- If we only use linear activation fucntions, out model wil lonly be able to predict linear relationships. We need to use non-linear functions.
- Note: ReLu is not normalized (output values can be anything >= 0)
- Softmax activation function - A greact activation function for classification 
- Softmax is capable of taking non-normalized data and produce a normalized distribution of probabilities for the classes
- This distribution represents the confidence scores for each class, adding up to 1  . The predicted class is chosen based on the output neuron with the largest confidence score
- The hyperbolic tan function almost always works better than the sigmoid function
- a = tanh(z) = (e^z - e^-z)/(e^z + e^-z). this ranges from -1 to 1
- It is essentially a rescaled and shifted version of the sigmoid function
- For hidden units, this is almost always better than sigmoid.
- with values between -1 and 1, the mean are closer to havign a zero mean.
- This 'center's the data closer to a mean of zero, improving the learning of the next layer
- The exception to this; the output layer. If y is between 0,1 , it makes sense for classification 0<y^<1
- We might use tanh for the hidden layer and sigmoid for the output for classification problems
- Downsides of tanh and sigmoid: if z is very large or small, the slope of the function is close to zero. This slows down learning 
- Relu works great here. The derivative is always 0 if z is negative 
- If you are using binary classification: sigmoid works alright for the output
- ReLu is increasingly the defualt activation function - it is used a lot!
- Leaky ReLu - instead of 0 when z is negative, there is a very slight slope
- Leaky ReLu usually actually works a bit better than ReLu, but its not used as much
- Never use sigmoid unless for the ouput layer when you are doing binary classification
- Tanh is strictly superior to sigmoid 
- ReLu is generally used the most 
- Leaky ReLu may be better than Relu ( a = max(0.01z,z) ) -> 0.01 can be a hyperparameter to tune 
- Testing different activation functions (as a development set of models) is a good idea to find the approrpiate one

Categorical Cross-Entropy Loss:

- Used to compare a "ground-truth" probability and some prediction distribution 
- Cross entropy compares two probability distributions 
- One of the most common loss functions used with the Softmax activation function (categorization) 
- one-hot : One value is 1, the rest are "off" (cold = 0) 
- This equation simplifies down to the negative log of the target classes confidence score 
- When confidence = 1 (100% sure about prediction), the loss = 0 

Optimization:

- Within this context, optimization is the process of tuning weights and biases to improve model performance.
- Its important to initialize the weights randomly to start with (rather than all zeros)
- This is specific for Neural networks ^
- a[1] = a[2] if all weights are 0 . Then dz[1] = dz[2]. The hidden units will be completely identical
- After every iteration of training, the hidden units will still be the same after each weight update 
- instead, initialize randomly. if we have 2 neruons in a layer, W[1] = np.random.randn((2,2)) * 0.01 (we want our weights to be random but very small)
- b[1] = np.zero((2,1) . biases dont suffer from the same issue
- We dont want Z to be too big in psoitive or neg direction. This will slow learning
- For very deep nerual networks, we might want a different constant than 0.01 
- This can be another hyperparameter to consider?

Derivatives of activation functions:

- Sigmoid: g(z) = 1/(1+e^-z)
- d/dz g(z) = slope of g(z) at z = 1/(1+e^-z)(1- 1/(1+e^-z)) = g(z)(1-(g(z))
- g'(z) = g(z)(1 - g(z))	= a(1-a)

- Tanh:  g(z) =tanh(z)
- g'(z) = 1 - (tanh(z))^2


- ReLu: g(z) = max(0,z)
- g'(z) = 0 if z<0  , 1 if z>0 .  undefined if z = 0 (this is not necessary to consider in practise)

- Leaky ReLU:
- g(z) = max(0.01z,z)
- g'(z) = 0.01 if z<0 , 1 if z > 0




Forward propagation:
-  Z[1] = W[1] X + b[1]
-  A[1] = g[1](z[1])
-  Z[2] = W[2] A[1] + b[2]
-  A[2] = g[2](z[2])

back propagation: binary classification with sigmoid 

-   dZ[2] = A[2] - Y 
-   dW[2] = 1/m dZ[2] A[1].T
-   db[2] = 1/m  np.sum(dZ[2], axis = 1, keepdims = True)
-   dZ[1] = W[2].T dZ[2] * g[1]'(Z[1])
-   dW[1] = 1/m  dZ[1] X.T
-   db[1] = 1/m  np.sum(dZ[1], axis = 1, keep dims = Ture)

- * = element wise product

Backprop intuition:
- Loss is defined as L(a,y) = -yloga - (1-y)log(1-a)
- d/da L =  - y/a + (1-y)/(1-a) 

-  dz = da * g'(z) 
-  a = sigmoid(z)

- dL/dz= dL/da * da/dz 
- da/dz = d/dz g(z) = g'(z)

- dw = dz * x 
- db = dz 

 Go back: calc dA[2], dZ[2] , dA[1], dZ[1]
- we can skip da[2]
- dz[2] = a[2] - y
- dw[2] = dz[2] a[1].T
- db[2] = dz[2] 

- dz[1] = W[2].t dz[2] * g[1]' (Z[1]) 
- W[2] = (n[2],n[1]) dimensions
- Z[2] , dZ[2] = (n[2],1) ---  (1,1) dimensions 
- dZ[1] = W[2].T dZ[2] * g[1]'(Z[1])
- (n[1],1) = (n[1],n[2])(n[2],1) * (n[1],1) dimensions 

- dW[1] = dz[1] X.T
db[1] = dz[1] 



The number of rows in W[k] is the number of neurons in the k-th layer and the number of columns is the number of inputs of the layer.